{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- The main purpose of this notebook is to process the 1000 .wav samples found in the local Data directory, slice them into smaller audio clips, convert these audio clips into mel-spectrograms, and convert the mel-spectrograms into numpy arrays.\n",
    "- Once the audio data is processed, it can be converted into a dataset. The data set is broken up into training data and testing data. \n",
    "- The training and testing data is then used to train our model, which is then saved in /backend/genre_categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = '../Data'\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_data(path_name):\n",
    "    \"\"\"Function to create paths/genre arrays to allow for\n",
    "    future conversion of audio files to mel_spectrograms.\"\"\"\n",
    "    paths, genres = [], []\n",
    "    for root, _, files in os.walk(path_name):\n",
    "        for name in files:\n",
    "            filename = os.path.join(root, name)\n",
    "            genre = os.path.split(root)[-1]\n",
    "            paths.append(filename)\n",
    "            genres.append(genre)\n",
    "    return paths, genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_melspectrogram(audio_file, sample_rate):\n",
    "    \"\"\" Function to create a mel_spectrogram from a received audio_file\"\"\"\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "          y=audio_file,\n",
    "          sr=sample_rate,\n",
    "          n_fft=2048,\n",
    "          hop_length=512,\n",
    "          n_mels=128)\n",
    "    # db = decibel units\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    return mel_spectrogram_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(mel_db_spect, max_length):\n",
    "    \"\"\" Pads mel_spectrogram files to ensure that they are homogeneous\n",
    "    in shape. \"\"\"\n",
    "    if mel_db_spect.shape[1] < max_length:\n",
    "        padding = max_length - mel_db_spect.shape[1]\n",
    "        mel_db_spect = np.pad(mel_db_spect, pad_width=((0,0), (0, padding)), mode='constant')\n",
    "    else:\n",
    "        mel_db_spect = mel_db_spect[:, :max_length]\n",
    "    return mel_db_spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_songs(audio_paths, genres, max_length=78):\n",
    "    \"\"\" Function that splits received songs into smaller audio clips and\n",
    "    converts them into mel-spectrograms to be used for training/prediction.\n",
    "    Adapted from code found at: \n",
    "    https://github.com/chittalpatel/Music-Genre-Classification-GTZAN/blob/master/Music%20Genre%20Classification/CNN_train(1).ipynb\n",
    "    \"\"\"\n",
    "    split_spects_mel_db = []\n",
    "    split_genres = []\n",
    "    window = 0.06\n",
    "    overlap = 0.3\n",
    "\n",
    "    for path, genre in tqdm(zip(audio_paths, genres), total=len(audio_paths),desc='Processing Audio Files'):  \n",
    "        audio, sample_rate = librosa.load(path)  \n",
    "        audio_shape = audio.shape[0]\n",
    "        chunk = int(audio_shape * window)\n",
    "        offset = int(chunk*(1 - overlap))\n",
    "        individual_split_song = []\n",
    "\n",
    "        # create array of smaller audio clips\n",
    "        for i in range(0, audio_shape - chunk + offset, offset):\n",
    "            individual_split_song.append(audio[i:i+chunk])\n",
    "        \n",
    "        # convert small clips into mel_spectrograms\n",
    "        for sample in individual_split_song:\n",
    "            if sample.shape[0] != chunk:\n",
    "                continue\n",
    "            mel_spec = librosa.feature.melspectrogram(y=sample, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=128)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Pad outputs to ensure uniformity \n",
    "            mel_spec_db = pad_audio(mel_spec_db, max_length)\n",
    "            \n",
    "            split_spects_mel_db.append(mel_spec_db)          \n",
    "            split_genres.append(genre)\n",
    "    \n",
    "    return split_spects_mel_db, split_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, gens = get_audio_data(DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Audio Files: 100%|██████████| 1000/1000 [01:49<00:00,  9.12it/s]\n"
     ]
    }
   ],
   "source": [
    "spects, gens2 = split_songs(paths, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine size of training set and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spects_len = len(spects)\n",
    "train_size = spects_len * .90\n",
    "test_size = spects_len - train_size\n",
    "train_take = train_size / 50\n",
    "test_take = test_size / 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = np.array(gens2)\n",
    "s1 = np.array(spects)\n",
    "genres_encoded = label_encoder.fit_transform(g1)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((s1, genres_encoded))\n",
    "dataset = dataset.shuffle(len(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = dataset.cache()\n",
    "final_data = final_data.batch(50)\n",
    "final_data = final_data.prefetch(25)\n",
    "train = final_data.take(int(train_take))\n",
    "test = final_data.skip(int(train_take)).take(int(test_take))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_height = 128\n",
    "spectrogram_width = 78\n",
    "num_channels = 1  #should be 1 because it is a numpy array, not a color image.\n",
    "num_classes = 10\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(spectrogram_height, spectrogram_width, num_channels)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "414/414 [==============================] - 101s 238ms/step - loss: 1.7050 - accuracy: 0.4501 - val_loss: 1.2181 - val_accuracy: 0.5730\n",
      "Epoch 2/15\n",
      "414/414 [==============================] - 74s 180ms/step - loss: 1.0748 - accuracy: 0.6238 - val_loss: 0.9186 - val_accuracy: 0.6874\n",
      "Epoch 3/15\n",
      "414/414 [==============================] - 74s 178ms/step - loss: 0.9157 - accuracy: 0.6797 - val_loss: 0.9054 - val_accuracy: 0.6765\n",
      "Epoch 4/15\n",
      "414/414 [==============================] - 72s 174ms/step - loss: 0.7959 - accuracy: 0.7273 - val_loss: 0.8741 - val_accuracy: 0.6943\n",
      "Epoch 5/15\n",
      "414/414 [==============================] - 72s 174ms/step - loss: 0.7032 - accuracy: 0.7602 - val_loss: 0.9708 - val_accuracy: 0.6657\n",
      "Epoch 6/15\n",
      "414/414 [==============================] - 74s 178ms/step - loss: 0.6342 - accuracy: 0.7838 - val_loss: 0.8060 - val_accuracy: 0.7252\n",
      "Epoch 7/15\n",
      "414/414 [==============================] - 76s 185ms/step - loss: 0.5598 - accuracy: 0.8052 - val_loss: 0.7430 - val_accuracy: 0.7504\n",
      "Epoch 8/15\n",
      "414/414 [==============================] - 78s 188ms/step - loss: 0.4929 - accuracy: 0.8301 - val_loss: 0.7623 - val_accuracy: 0.7639\n",
      "Epoch 9/15\n",
      "414/414 [==============================] - 75s 182ms/step - loss: 0.4397 - accuracy: 0.8500 - val_loss: 0.7343 - val_accuracy: 0.7739\n",
      "Epoch 10/15\n",
      "414/414 [==============================] - 78s 188ms/step - loss: 0.3910 - accuracy: 0.8652 - val_loss: 0.8135 - val_accuracy: 0.7578\n",
      "Epoch 11/15\n",
      "414/414 [==============================] - 75s 180ms/step - loss: 0.3354 - accuracy: 0.8810 - val_loss: 1.1233 - val_accuracy: 0.7078\n",
      "Epoch 12/15\n",
      "414/414 [==============================] - 73s 175ms/step - loss: 0.2970 - accuracy: 0.8937 - val_loss: 0.9441 - val_accuracy: 0.7496\n",
      "Epoch 13/15\n",
      "414/414 [==============================] - 73s 177ms/step - loss: 0.2622 - accuracy: 0.9062 - val_loss: 0.9477 - val_accuracy: 0.7622\n",
      "Epoch 14/15\n",
      "414/414 [==============================] - 74s 180ms/step - loss: 0.2277 - accuracy: 0.9204 - val_loss: 0.9053 - val_accuracy: 0.7665\n",
      "Epoch 15/15\n",
      "414/414 [==============================] - 77s 187ms/step - loss: 0.2016 - accuracy: 0.9276 - val_loss: 1.2706 - val_accuracy: 0.7183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x284e39c33d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, epochs=15, validation_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../genre_categorization\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../genre_categorization\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../genre_categorization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
